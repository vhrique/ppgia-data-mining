{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bibliotecas\n",
    "As seguintes bibliotecas são carregadas:\n",
    "- **Pandas:** Tratamento de dados\n",
    "- **Numpy:** Tratamento de dados\n",
    "- **Matplotlib.pyplot:** Gráficos\n",
    "- **Sci-kit:** Aprendizado de Máquina\n",
    "  - **decomposition:** Redução de dimensionalidade (PCA)\n",
    "  - **svm:** Support Vector Machine\n",
    "  - **tree:** Decision Tree\n",
    "  - **neighbors:** Nearest Neighbors\n",
    "  - **train_test_split:** Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deap\n",
      "  Downloading deap-1.2.2.tar.gz (936kB)\n",
      "\u001b[K    100% |████████████████████████████████| 942kB 625kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: deap\n",
      "  Running setup.py bdist_wheel for deap ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/82/aa/67/2c93e17c84646c86099fda53ee0b3329372dcf94dd8789fd13\n",
      "Successfully built deap\n",
      "Installing collected packages: deap\n",
      "Successfully installed deap-1.2.2\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import array, random\n",
    "from deap import creator, base, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dados de treinamento e teste a partir de arquivo CSV\n",
    "training = pd.read_csv('Data/aps_failure_training_set.csv', na_values='na')\n",
    "test = pd.read_csv('Data/aps_failure_test_set.csv', na_values='na')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma coluna \"Classe\" em tipo Lógico\n",
    "training['class'] = training['class'] != 'neg'\n",
    "test['class'] = test['class'] != 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verificar Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing: atribuição de dados faltantes (utilizando valor médio)\n",
    "for i in range(2, training.shape[1]):\n",
    "    training.iloc[:,i] = training.iloc[:,i].fillna(training.iloc[:,i].mean())\n",
    "    test.iloc[:,i] = test.iloc[:,i].fillna(test.iloc[:,i].mean())\n",
    "    \n",
    "# TODO: testar outros métodos de imputing (moda, mediana, KNN, regressão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar média e desvio padrão a partir de features de histograma\n",
    "def hist_features(data, column_name):\n",
    "    \n",
    "    # Média\n",
    "    weights = np.array([1, 2, 3, 4, 5, 6, 7, 8 , 9,  10])\n",
    "    counts = data.loc[:, (column_name + '_000'):(column_name + '_009')]\n",
    "    means = np.array(np.sum(counts * weights, axis = 1) / counts.sum(axis = 1))\n",
    "        \n",
    "    mean_data = pd.DataFrame(means, columns=[column_name + '_mean'])\n",
    "    \n",
    "    # Desvio-padrão\n",
    "    weights_matrix = np.array([weights,] * data.shape[0])\n",
    "    differences = (weights_matrix.transpose() - means.transpose()).transpose()\n",
    "    stds = np.sqrt(np.sum((differences ** 2) * counts, axis = 1) / counts.sum(axis=1))\n",
    "        \n",
    "    std_data = pd.DataFrame(stds, columns=[column_name + '_std'])\n",
    "    \n",
    "    # Novas colunas\n",
    "    new_data = pd.concat([mean_data, std_data], axis=1)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar novas features com função anterior\n",
    "column_names = ['ag', 'ay', 'az', 'ba', 'cn', 'cs', 'ee']\n",
    "\n",
    "for column_name in column_names:\n",
    "    \n",
    "    new_training = hist_features(training, column_name)\n",
    "    new_test = hist_features(test, column_name)\n",
    "\n",
    "    training = pd.concat([training, new_training], axis=1)\n",
    "    test = pd.concat([test, new_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar dados faltantes de novas features\n",
    "training[pd.isnull(training)] = 0\n",
    "test[pd.isnull(test)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample: balanceamento de categorias\n",
    "\n",
    "# Separa todas as observações verdadeiras\n",
    "true = training.loc[training['class']][:]\n",
    "\n",
    "# Separa todas as observações falsas\n",
    "false = training.loc[~training['class']][:]\n",
    "\n",
    "# Separa uma amostra aleatória das observações falsas (10%)\n",
    "false_sample = false.sample(frac = 0.1)\n",
    "\n",
    "# Concatena observações\n",
    "undersample = pd.concat([true, false_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar dados de treinamento e validação (70% e 30%)\n",
    "training_data, validation_data = train_test_split(undersample, test_size=0.3)\n",
    "\n",
    "X_train = training_data.loc[:, training_data.columns != 'class']\n",
    "Y_train = training_data.loc[:, 'class']\n",
    "\n",
    "X_val = validation_data.loc[:, validation_data.columns != 'class']\n",
    "Y_val = validation_data.loc[:, 'class']\n",
    "\n",
    "# Preparar dados de teste\n",
    "X_test = test.loc[:, test.columns != 'class']\n",
    "Y_test = test.loc[:, 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=30, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redução de dimensionalidade (com principal component analysis)\n",
    "pca = decomposition.PCA(n_components = 30) # Diminuindo de 182 para 30 atributos\n",
    "\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Não será realizado, pois descaracteriza \"Data Mining\".\n",
    "#X_train = pd.DataFrame(pca.transform(X_train))\n",
    "#X_val = pd.DataFrame(pca.transform(X_val))\n",
    "#X_test = pd.DataFrame(pca.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar dados por normalização\n",
    "media = X_train.mean()\n",
    "desvio_padrao = X_train.std()\n",
    "\n",
    "X_train = (X_train - media) / desvio_padrao\n",
    "X_val = (X_val - media) / desvio_padrao\n",
    "X_test = (X_test - media) / desvio_padrao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover classe sem variância (estava criando erros devido a divisão por zero da normalização)\n",
    "classe_sem_variancia = desvio_padrao == 0\n",
    "\n",
    "X_train = X_train.loc[:, ~classe_sem_variancia]\n",
    "X_val = X_val.loc[:, ~classe_sem_variancia]\n",
    "X_test = X_test.loc[:, ~classe_sem_variancia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento e validação de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia modelo de classificação\n",
    "#model = svm.SVC(kernel='rbf')\n",
    "model = tree.DecisionTreeClassifier(random_state=0)\n",
    "#model = neighbors.KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31510, 271, 51, 62, 1686]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previsão com dados de validação\n",
    "y = model.predict(X_val)\n",
    "\n",
    "# Analizar resultado\n",
    "tp = sum(Y_val & y)\n",
    "tn = sum(~Y_val & ~y)\n",
    "fp = sum(~Y_val & y)\n",
    "fn = sum(Y_val & ~y)\n",
    "\n",
    "score = 10 * fp + 500 * fn\n",
    "\n",
    "[score, tp, fp, fn, tn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teste de modelo\n",
    "**Objetivo:** Score < 9920."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29430, 325, 443, 50, 15182]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previsão com dados de teste\n",
    "y = model.predict(X_test)\n",
    "\n",
    "# Analizar resultado\n",
    "tp = sum(Y_test & y)\n",
    "tn = sum(~Y_test & ~y)\n",
    "fp = sum(~Y_test & y)\n",
    "fn = sum(Y_test & ~y)\n",
    "\n",
    "score = 10 * fp + 500 * fn\n",
    "\n",
    "[score, tp, fp, fn, tn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimização de modelo por seleção de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para otimização\n",
    "def my_evaluation(x):\n",
    "    \n",
    "    features = np.asarray(x) == 1\n",
    "    features = list(features)\n",
    "\n",
    "    # Inicializa modelo\n",
    "    model = tree.DecisionTreeClassifier(random_state=0)\n",
    "    #model = svm.SVC()\n",
    "\n",
    "    # Treinamento com atributos selecionados\n",
    "    model.fit(X_train.loc[:, features], Y_train)\n",
    "\n",
    "    # Realiza previsão de dados de validação\n",
    "    y = model.predict(X_val.loc[:, features])\n",
    "\n",
    "    # Analizar resultado\n",
    "    tp = sum(Y_val & y)\n",
    "    tn = sum(~Y_val & ~y)\n",
    "    fp = sum(~Y_val & y)\n",
    "    fn = sum(Y_val & ~y)\n",
    "    score = 10 * fp + 500 * fn\n",
    "    \n",
    "    # Retorna score do modelo\n",
    "    return (score,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/deap/creator.py:141: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/deap/creator.py:141: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "creator.create(\"FitnessMin\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "toolbox.register(\"bit\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.bit, 183)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "#evalOneMax = lambda individual: (sum(individual),)\n",
    "\n",
    "toolbox.register(\"evaluate\", my_evaluation)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=10)\n",
    "\n",
    "population = toolbox.population(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGEN=100\n",
    "for gen in range(NGEN):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1)\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    population = offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23500.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual = tools.selWorst(population, 1, fit_attr='fitness')[0]\n",
    "features = list(np.asarray(individual) == 1)\n",
    "\n",
    "individual.fitness.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de modelo otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31700, 319, 370, 56, 15255]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação e treinamento de modelo\n",
    "model = tree.DecisionTreeClassifier(random_state=0)\n",
    "#model = svm.SVC()\n",
    "model.fit(X_train.loc[:, features], Y_train)\n",
    "\n",
    "# Teste de modelo\n",
    "# Previsão com dados de teste\n",
    "y = model.predict(X_test.loc[:, features])\n",
    "\n",
    "# Analizar resultado\n",
    "tp = sum(Y_test & y)\n",
    "tn = sum(~Y_test & ~y)\n",
    "fp = sum(~Y_test & y)\n",
    "fn = sum(Y_test & ~y)\n",
    "\n",
    "score = 10 * fp + 500 * fn\n",
    "\n",
    "[score, tp, fp, fn, tn]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
